{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93be197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5c9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceWeightedKNN():\n",
    "    \"\"\"Unsupervised learner for implementing neighbor searches.\n",
    "    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
    "    .. versionadded:: 0.9\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "    radius : float, default=1.0\n",
    "        Range of parameter space to use by default for :meth:`radius_neighbors`\n",
    "        queries.\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm\n",
    "          based on the values passed to :meth:`fit` method.\n",
    "        Note: fitting on sparse input will override the setting of\n",
    "        this parameter, using brute force.\n",
    "    leaf_size : int, default=30\n",
    "        Leaf size passed to BallTree or KDTree.  This can affect the\n",
    "        speed of the construction and query, as well as the memory\n",
    "        required to store the tree.  The optimal value depends on the\n",
    "        nature of the problem.\n",
    "    metric : str or callable, default='minkowski'\n",
    "        The distance metric to use for the tree.  The default metric is\n",
    "        minkowski, and with p=2 is equivalent to the standard Euclidean\n",
    "        metric. For a list of available metrics, see the documentation of\n",
    "        :class:`~sklearn.metrics.DistanceMetric`.\n",
    "        If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
    "        must be square during fit. X may be a :term:`sparse graph`,\n",
    "        in which case only \"nonzero\" elements may be considered neighbors.\n",
    "    p : int, default=2\n",
    "        Parameter for the Minkowski metric from\n",
    "        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
    "        equivalent to using manhattan_distance (l1), and euclidean_distance\n",
    "        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "    metric_params : dict, default=None\n",
    "        Additional keyword arguments for the metric function.\n",
    "    n_jobs : int, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    Attributes\n",
    "    ----------\n",
    "    effective_metric_ : str\n",
    "        Metric used to compute distances to neighbors.\n",
    "    effective_metric_params_ : dict\n",
    "        Parameters for the metric used to compute distances to neighbors.\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "        .. versionadded:: 0.24\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "        .. versionadded:: 1.0\n",
    "    n_samples_fit_ : int\n",
    "        Number of samples in the fitted data.\n",
    "    See Also\n",
    "    --------\n",
    "    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n",
    "        vote.\n",
    "    RadiusNeighborsClassifier : Classifier implementing a vote among neighbors\n",
    "        within a given radius.\n",
    "    KNeighborsRegressor : Regression based on k-nearest neighbors.\n",
    "    RadiusNeighborsRegressor : Regression based on neighbors within a fixed\n",
    "        radius.\n",
    "    BallTree : Space partitioning data structure for organizing points in a\n",
    "        multi-dimensional space, used for nearest neighbor search.\n",
    "    Notes\n",
    "    -----\n",
    "    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
    "    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
    "    https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.neighbors import NearestNeighbors\n",
    "    >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
    "    >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
    "    >>> neigh.fit(samples)\n",
    "    NearestNeighbors(...)\n",
    "    >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n",
    "    array([[2, 0]]...)\n",
    "    >>> nbrs = neigh.radius_neighbors(\n",
    "    ...    [[0, 0, 1.3]], 0.4, return_distance=False\n",
    "    ... )\n",
    "    >>> np.asarray(nbrs[0][0])\n",
    "    array(2)\n",
    "    \"\"\"    \n",
    "    \n",
    "    #constructor\n",
    "    def __init__(self, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.radius = radius\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.metric_params = metric_params\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    #fit\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        #jumlah sampel, jumlah fitur\n",
    "        self.m, self.n = X_train.shape\n",
    "        \n",
    "    #predict\n",
    "    def predict(self, X_test):\n",
    "        self.X_test = X_test\n",
    "        \n",
    "        #jumlah sampel, jumlah fitur\n",
    "        self.m_test, self.n = X_test.shape\n",
    "        \n",
    "        y_predict = np.zeros(self.m_test)\n",
    "    \n",
    "        nn = NearestNeighbors(n_neighbors=self.n_neighbors, radius=self.radius, algorithm=self.algorithm, leaf_size=self.leaf_size, metric=self.metric, p=self.p, metric_params=self.metric_params, n_jobs=self.n_jobs).fit(self.X_train).fit(self.X_train)\n",
    "        dist, ind = nn.kneighbors(self.X_test)\n",
    "        \n",
    "        label_predicted = np.zeros(len(dist))\n",
    "        for i in range(len(dist)):\n",
    "            dst = dist[i]\n",
    "            idx = ind[i]\n",
    "            \n",
    "            w = np.zeros(self.n_neighbors)\n",
    "            for j in range(self.n_neighbors):\n",
    "                if(np.amax(dst) == np.amin(dst)):\n",
    "                    w[j] = 1\n",
    "                else:\n",
    "                    w[j] = (np.amax(dst)-dst[j])/(np.amax(dst)-np.amin(dst))*(np.amax(dst)+np.amin(dst))/(np.amax(dst)+dst[j])\n",
    "            \n",
    "            y_train_sorted = self.y_train[idx]\n",
    "            listed = defaultdict(list)\n",
    "            for label, bobot in zip(y_train_sorted, w):\n",
    "                listed[label].append(bobot)\n",
    "            \n",
    "            for label, bobot in listed.items():\n",
    "                listed[label] = mean(bobot)\n",
    "            \n",
    "            label_predicted[i] = max(listed, key=listed.get)\n",
    "            \n",
    "        return label_predicted\n",
    "    \n",
    "    #score\n",
    "    def score(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        #predict\n",
    "        y_predict = self.predict(self.X)\n",
    "        \n",
    "        #hitung akurasi\n",
    "        cor = []\n",
    "        n_correct = 0\n",
    "        for act, pred in zip(self.y, y_predict):\n",
    "            if act == pred:\n",
    "                n_correct += 1\n",
    "        \n",
    "        return n_correct / self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02333ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jcopml]",
   "language": "python",
   "name": "conda-env-jcopml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
